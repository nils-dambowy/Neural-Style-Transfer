{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXIeSYVPRcON"
      },
      "source": [
        "# Neural Style Transfer using Deep Learning\n",
        "\n",
        "In this notebook, we implement the Neural Style Transfer algorithm based on the paper \"A Neural Algorithm of Artistic Style\" by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.\n",
        "\n",
        "## Introduction\n",
        "Neural Style Transfer (NST) is a technique that takes two images—a content image and a style image—and blends them together so that the output image looks like the content image but \"painted\" in the style of the style image.\n",
        "\n",
        "The algorithm uses a pretrained Convolutional Neural Network (CNN), typically VGG19, to extract features from both the content and style images. The core idea is to match the content representation of the output image with that of the content image and the style representation of the output image with that of the style image.\n",
        "\n",
        "### Loss Functions\n",
        "The NST algorithm optimizes the output image by minimizing a loss function that has two components:\n",
        "- **Content Loss**: Measures the difference in content between the output image and the content image.\n",
        "- **Style Loss**: Measures the difference in style between the output image and the style image using the Gram matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RyGY0H7RRcOQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2h4ZLdFRcOR"
      },
      "source": [
        "### Defining the VGG Model\n",
        "We use a pretrained VGG19 model for feature extraction. Since Layer 29 and onwards of the model consist of FC layers, we cut them off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bOmTPfHKRcOS"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.chosen_features = ['0', '5', '10', '19', '28']\n",
        "        self.model = models.vgg19(pretrained=True).features[:29]\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for layer_num, layer in enumerate(self.model):\n",
        "            x = layer(x)\n",
        "            if str(layer_num) in self.chosen_features:\n",
        "                features.append(x)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRme74RpRcOT"
      },
      "source": [
        "### Loading and Preprocessing Images\n",
        "We define a function to load and preprocess the images. The images are resized and normalized to be compatible with the pretrained VGG model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qgPoFRapRcOT"
      },
      "outputs": [],
      "source": [
        "def load_image(image_name):\n",
        "  image = Image.open(image_name)\n",
        "  image = loader(image).unsqueeze(0)\n",
        "  return image.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sX3WRMhRcOU"
      },
      "source": [
        "### Loading Images and Initializing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxJoMZzlRcOU",
        "outputId": "6e78a1fa-d89a-4ccb-e97f-d357c20c7166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:03<00:00, 164MB/s]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_size = 512\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "original_img = load_image(\"input.png\")\n",
        "style_img = load_image(\"style.jpg\")\n",
        "\n",
        "model = VGG().to(device).eval()\n",
        "generated = original_img.clone().requires_grad_(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3P2NJRaRcOV"
      },
      "source": [
        "### Defining the Loss Functions\n",
        "The content loss and style loss functions are defined as described in the paper. The content loss is the mean squared error between the feature maps of the generated and content images. The style loss is the mean squared error between the Gram matrices of the feature maps of the generated and style images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "awbAVh_xRcOW"
      },
      "outputs": [],
      "source": [
        "def content_loss(generated_feature, content_feature):\n",
        "    return torch.mean((generated_feature - content_feature) ** 2)\n",
        "\n",
        "def gram_matrix(feature):\n",
        "    _, n_channels, height, width = feature.size()\n",
        "    feature = feature.view(n_channels, height * width)\n",
        "    G = torch.mm(feature, feature.t())\n",
        "    return G / (n_channels * height * width)\n",
        "\n",
        "def style_loss(generated_feature, style_feature):\n",
        "    G = gram_matrix(generated_feature)\n",
        "    A = gram_matrix(style_feature)\n",
        "    return torch.mean((G - A) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-6jpImKRcOX"
      },
      "source": [
        "### Optimizing the Generated Image\n",
        "We perform gradient descent on the generated image to minimize the combined content and style loss. The total loss is a weighted sum of the content and style losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukom3UhARcOY",
        "outputId": "d26ab610-f249-4e12-d83f-6862b7ad70e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [0/10000], Content Loss: 0.0001, Style Loss: 323228064.0000, Total Loss: 3232280.5000\n",
            "Step [200/10000], Content Loss: 12.5102, Style Loss: 27729052.0000, Total Loss: 277303.0000\n",
            "Step [400/10000], Content Loss: 13.0549, Style Loss: 16320842.0000, Total Loss: 163221.4844\n",
            "Step [600/10000], Content Loss: 13.2986, Style Loss: 8429695.0000, Total Loss: 84310.2422\n",
            "Step [800/10000], Content Loss: 13.4644, Style Loss: 3846746.2500, Total Loss: 38480.9258\n",
            "Step [1000/10000], Content Loss: 13.5957, Style Loss: 2248197.5000, Total Loss: 22495.5703\n",
            "Step [1200/10000], Content Loss: 13.7000, Style Loss: 1721329.0000, Total Loss: 17226.9883\n"
          ]
        }
      ],
      "source": [
        "total_steps = 10000\n",
        "learning_rate = 0.001\n",
        "alpha = 1\n",
        "beta = 0.01\n",
        "optimizer = optim.Adam([generated], lr=learning_rate)\n",
        "\n",
        "for step in range(total_steps):\n",
        "    # extract features\n",
        "    generated_features = model(generated)\n",
        "    original_img_features = model(original_img)\n",
        "    style_img_features = model(style_img)\n",
        "\n",
        "    c_loss = 0\n",
        "    s_loss = 0\n",
        "\n",
        "    for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_img_features):\n",
        "        batch_size, channel, height, width = gen_feature.shape\n",
        "        c_loss += torch.mean((gen_feature - orig_feature)**2)\n",
        "        # compute gram matrix\n",
        "        G = gen_feature.view(channel, height*width).mm(\n",
        "            gen_feature.view(channel, height*width).t()\n",
        "        )\n",
        "\n",
        "        A = style_feature.view(channel, height*width).mm(\n",
        "            style_feature.view(channel, height*width).t()\n",
        "        )\n",
        "\n",
        "        s_loss += torch.mean((G-A)**2)\n",
        "\n",
        "    total_loss = alpha * c_loss + beta * s_loss\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 200 == 0:\n",
        "        print(f'Step [{step}/{total_steps}], Content Loss: {c_loss.item():.4f}, Style Loss: {s_loss.item():.4f}, Total Loss: {total_loss.item():.4f}')\n",
        "        save_image(generated, f\"generated_{step}.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}