{
<<<<<<< HEAD
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Neural Style Transfer using Deep Learning\n",
                "\n",
                "In this notebook, we implement the Neural Style Transfer algorithm based on the paper \"A Neural Algorithm of Artistic Style\" by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.\n",
                "\n",
                "## Introduction\n",
                "Neural Style Transfer (NST) is a technique that takes two images—a content image and a style image—and blends them together so that the output image looks like the content image but \"painted\" in the style of the style image.\n",
                "\n",
                "The algorithm uses a pretrained Convolutional Neural Network (CNN), typically VGG19, to extract features from both the content and style images. The core idea is to match the content representation of the output image with that of the content image and the style representation of the output image with that of the style image.\n",
                "\n",
                "### Loss Functions\n",
                "The NST algorithm optimizes the output image by minimizing a loss function that has two components:\n",
                "- **Content Loss**: Measures the difference in content between the output image and the content image.\n",
                "- **Style Loss**: Measures the difference in style between the output image and the style image using the Gram matrix.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from PIL import Image\n",
                "import torchvision.transforms as transforms\n",
                "import torchvision.models as models\n",
                "from torchvision.utils import save_image"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Defining the VGG Model\n",
                "We use a pretrained VGG19 model for feature extraction. Since Layer 29 and onwards of the model consist of FC layers, we cut them off"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VGG(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(VGG, self).__init__()\n",
                "        self.chosen_features = ['0', '5', '10', '19', '28']\n",
                "        self.model = models.vgg19(pretrained=True).features[:29]\n",
                "\n",
                "    def forward(self, x):\n",
                "        features = []\n",
                "        for layer_num, layer in enumerate(self.model):\n",
                "            x = layer(x)\n",
                "            if str(layer_num) in self.chosen_features:\n",
                "                features.append(x)\n",
                "        return features"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Loading and Preprocessing Images\n",
                "We define a function to load and preprocess the images. The images are resized and normalized to be compatible with the pretrained VGG model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_image(image_name):\n",
                "  image = Image.open(image_name)\n",
                "  image = loader(image).unsqueeze(0)\n",
                "  return image.to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Loading Images and Initializing the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\ndamb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
                        "  warnings.warn(\n",
                        "c:\\Users\\ndamb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
                        "  warnings.warn(msg)\n"
                    ]
                }
            ],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "image_size = 512\n",
                "loader = transforms.Compose([\n",
                "    transforms.Resize((image_size, image_size)),\n",
                "    transforms.ToTensor()\n",
                "])\n",
                "\n",
                "original_img = load_image(\"input.png\")\n",
                "style_img = load_image(\"style.jpg\")\n",
                "\n",
                "model = VGG().to(device).eval()\n",
                "generated = original_img.clone().requires_grad_(True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Defining the Loss Functions\n",
                "The content loss and style loss functions are defined as described in the paper. The content loss is the mean squared error between the feature maps of the generated and content images. The style loss is the mean squared error between the Gram matrices of the feature maps of the generated and style images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def content_loss(generated_feature, content_feature):\n",
                "    return torch.mean((generated_feature - content_feature) ** 2)\n",
                "\n",
                "def gram_matrix(feature):\n",
                "    _, n_channels, height, width = feature.size()\n",
                "    feature = feature.view(n_channels, height * width)\n",
                "    G = torch.mm(feature, feature.t())\n",
                "    return G / (n_channels * height * width)\n",
                "\n",
                "def style_loss(generated_feature, style_feature):\n",
                "    G = gram_matrix(generated_feature)\n",
                "    A = gram_matrix(style_feature)\n",
                "    return torch.mean((G - A) ** 2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Optimizing the Generated Image\n",
                "We perform gradient descent on the generated image to minimize the combined content and style loss. The total loss is a weighted sum of the content and style losses."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_steps = 10000\n",
                "learning_rate = 0.001\n",
                "alpha = 1\n",
                "beta = 0.01\n",
                "optimizer = optim.Adam([generated], lr=learning_rate)\n",
                "\n",
                "for step in range(total_steps):\n",
                "    # extract features\n",
                "    generated_features = model(generated)\n",
                "    original_img_features = model(original_img)\n",
                "    style_img_features = model(style_img)\n",
                "\n",
                "    c_loss = 0\n",
                "    s_loss = 0\n",
                "\n",
                "    for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_img_features):\n",
                "        c_loss += content_loss(gen_feature, orig_feature)\n",
                "        s_loss += style_loss(gen_feature, style_feature)\n",
                "\n",
                "    total_loss = alpha * c_loss + beta * s_loss\n",
                "    optimizer.zero_grad()\n",
                "    total_loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    if step % 200 == 0:\n",
                "        print(f'Step [{step}/{total_steps}], Content Loss: {c_loss.item():.4f}, Style Loss: {s_loss.item():.4f}, Total Loss: {total_loss.item():.4f}')\n",
                "        save_image(generated, f\"generated_{step}.png\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
=======
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m\n\u001b[0;32m     23\u001b[0m image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[0;32m     24\u001b[0m loader \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     25\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((image_size, image_size)),\n\u001b[0;32m     26\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m     27\u001b[0m ])\n\u001b[1;32m---> 29\u001b[0m original_img \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m style_img \u001b[38;5;241m=\u001b[39m load_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m VGG()\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(image_name)\u001b[0m\n\u001b[0;32m     17\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_name)\n\u001b[0;32m     18\u001b[0m image \u001b[38;5;241m=\u001b[39m loader(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ndamb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "class VGG(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(VGG, self).__init__()\n",
    "\n",
    "    self.chosen_features = ['0', '5', '10', '19', '28']\n",
    "    self.model = models.vgg19(weights=models.VGG19_Weights).features[:29]\n",
    "\n",
    "  def forward(self, x):\n",
    "    features = []\n",
    "    for layer_num, layer in enumerate(self.model):\n",
    "      x = layer(x)\n",
    "      if str(layer_num) in self.chosen_features:\n",
    "        features.append(x)\n",
    "    return features\n",
    "  \n",
    "def load_image(image_name):\n",
    "  image = Image.open(image_name)\n",
    "  image = loader(image).unsqueeze(0)\n",
    "  return image.to(device)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "image_size = 512\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "original_img = load_image(\"input.png\")\n",
    "style_img = load_image(\"style.jpg\")\n",
    "\n",
    "model = VGG().to(device).eval()\n",
    "# take a copy of the orignal image\n",
    "generated = original_img.clone().requires_grad_(True)\n",
    "\n",
    "# hyperparameters\n",
    "total_steps = 10000\n",
    "learning_rate = 0.001\n",
    "alpha =  1\n",
    "beta = 0.01\n",
    "optimizer = optim.Adam([generated], lr=learning_rate,)\n",
    "\n",
    "for step in range(total_steps):\n",
    "  generated_features = model(generated)\n",
    "  original_img_features = model(original_img)\n",
    "  style_img_features = model(style_img)\n",
    "\n",
    "  content_loss = style_loss = 0\n",
    "\n",
    "  for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_img_features):\n",
    "    batch_size, channel, height, width = gen_feature.shape\n",
    "    content_loss += torch.mean((gen_feature - orig_feature)**2)\n",
    "\n",
    "    # compute gram matrix\n",
    "    G = gen_feature.view(channel, height*width).mm(\n",
    "        gen_feature.view(channel, height*width).t()\n",
    "    )\n",
    "\n",
    "    A = style_feature.view(channel, height*width).mm(\n",
    "        style_feature.view(channel, height*width).t()\n",
    "    )\n",
    "\n",
    "    style_loss += torch.mean((G-A)**2)\n",
    "\n",
    "  total_loss = alpha*content_loss + beta*style_loss\n",
    "  optimizer.zero_grad()\n",
    "  total_loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if step % 200 == 0:\n",
    "    print(total_loss)\n",
    "    save_image(generated, \"{s}.png\".format(s=step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
>>>>>>> 456f0688683441478667fa7947e97cf73019eb7b
}
